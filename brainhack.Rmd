---
title: "Examples for Brainhack day"
output:
  html_document:
    df_print: paged
---

This notebook contains a few examples of how to fit some basic models, such as logistic and softmax regression, using ```rstanarm``` and ```rstan```.

We first load some required libraries. If you do not have these libraries installed, you can install them from CRAN using the command ```install.packages```, for example ```install.packages('rstan')```. The only exception is the package ```dimreduce``` which is not in CRAN, but you can install it using the command using the command on this page https://github.com/jpiironen/dimreduce.

```{r, message=FALSE}
library(rstan)
library(rstanarm)
library(ggplot2)
library(dimreduce)
options(mc.cores=parallel::detectCores()) # this command will automatically utilize multicore-cpu
```


# Logistic regression

Let's load a binary classification dataset that has 410 features and 86 observations
```{r}
load(file='data/schizophrenia.RData')
n <- nrow(schizophrenia)
d <- ncol(schizophrenia)-1
x <- schizophrenia[,2:(d+1)]
y <- schizophrenia[,1]
```


In the spirit of a prediction contest, we split the dataset into training and test sets.
```{r}
set.seed(323413)
ntest <- 26
itest <- sample(1:n, ntest)
itrain <- setdiff(1:n, itest)
xtest <- x[itest,]
ytest <- y[itest]
x <- x[itrain,]
y <- y[itrain]
n <- length(y)
```


For illustration, visualize the dataset using the first two supervised principal components (see Piironen and Vehtari, 2018)
```{r fig.width=4, fig.height=3, message=FALSE, warning=FALSE, results='hide'}
dr <- spca(x,y, nctot=20, alpha = 0.05) # supervised principal components (SPC)
# dr <- ispca(x,y, nctot=20, alpha=0.1) # iterative SPC, alternative dimension reduction technique
z <- dr$z
ggplot() + geom_point(aes(x=z[,1],y=z[,2]), color=y+2)
```

The classes are overlapping but especially the first component is predictive about the class label.


The logistic regression model could be fit either using the original features ```x``` or the transformed features ```z```. For high-dimensional problems (where the number of features is several hundreds or more), model fitting using the original features often becomes computationally slow. It is therefore typically a good strategy to start simple and try fitting the model using the reduced set of features as we shall do here. The following command fits Bayesian logistic regression model using the regularized horseshoe prior (Piironen and Vehtari, 2017) on the regression coefficients.
```{r, message=F}
fit <- stan_glm(y~., family='binomial', data=data.frame(z,y), 
                prior = hs(global_scale = 1/(ncol(z)-1)/sqrt(n)), iter=500)
```
Compute the predictions on the test set and check the classification accuracy
```{r}
ztest <- predict(dr, xtest) # the transformed features corresponding to the test x
pred <- colMeans(posterior_linpred(fit, newdata = data.frame(ztest), transform = T))
mean(round(pred)==ytest)
```

For illustration, let's fit the model using the original features, which takes a bit longer due to the high-dimensional feature space
```{r, warning=F, message=F}
fit2 <- stan_glm(y~., family='binomial', data=data.frame(x,y), prior=hs(global_scale = 1/(d-1)/sqrt(n), slab_df = 7), iter=500)
```

Predictions on the test data
```{r}
pred <- colMeans(posterior_linpred(fit2, newdata = data.frame(xtest), transform = T))
mean(round(pred)==ytest)
```
In this case the accuracy on the test data seems to be exactly the same for the two models. (Notice also that even a seemingly large difference might not be statistically different considering that the test set has only 26 instances, so a single misclassification would decrease the accuracy by 1/26 = 0.038.)









## Challenge dataset

This section contains an example fit to the challenge dataset. Let's load the data first. Below we use all variables except the class label as predictor variables. The categorical scannerID is split into three binary features.
```{r}
# read the label data
dat <- read.csv(file='data/pac2018/phenotype.csv', header = T, sep=',')
y <- dat$label - 1 # 0 or 1
x1 <- data.frame(id1=as.numeric(dat$scannerID==1),
                id2=as.numeric(dat$scannerID==2),
                id3=as.numeric(dat$scannerID==3))
x1 <- cbind(x1, gender=dat$gender-1, age=dat$age, TIV=dat$TIV )

# read the sMRI data
x2 <- read.csv(file='data/pac2018/sMRI.csv', header = T, sep=',')
```

It is often advisable to normalize the features to have a zero mean and unit variance if the original scales are very different. Here we normalize all but the binary features
```{r}
x1[,c('age','TIV')] <- scale(x1[,c('age','TIV')])
x2 <- scale(x2)
```


Let's split the data into training and validation sets.
```{r}
set.seed(1234)
ntest <- round(0.3*length(y))
itest <- sample(1:length(y), ntest)
itrain <- setdiff(1:length(y), itest)
x1test <- x1[itest,]
x2test <- x2[itest,]
ytest <- y[itest]
x1 <- x1[itrain,]
x2 <- x2[itrain,]
y <- y[itrain]
n <- length(y)
```


As the dimensionality of the sMRI data is quite high, we again apply dimension reduction to it. Notice though that we leave the other features untouched.
```{r, message=F}
dr <- spca(x2,y, nctot=20, ncsup=10, alpha = 0.01, normalize = F) # supervised principal components (SPC)
z <- dr$z
```
Here's the 2d visualization of the data using the sMRI data
```{r, fig.width=4, fig.height=3}
ggplot() + geom_point(aes(x=z[,1],y=z[,2]), color=y+2)
```

The first two latent features seem only weakly predictive about the class label, hinting that the sMRI data is not very strongly predictive about the class label.

Let's fit the logistic regression model using the confounding features and the new features extracted from the sMRI data
```{r, message=F}
fit <- stan_glm(y~., family='binomial', data=data.frame(x1,z,y), 
                prior = hs(global_scale = 1/(ncol(z)-1)/sqrt(n)), iter=300)
```


```{r}
ztest <- predict(dr, x2test) # the transformed features corresponding to the test x
pred <- colMeans(posterior_linpred(fit, newdata = data.frame(x1test,ztest), transform = T))
mean(round(pred)==ytest)
```
The model gets only about 65% classification accuracy on the test/validation data, which is not much, but still better than always predicting the majority class (about 57% of the observations belong to class ```y=0```).


To get some insight which of the features are relevant for prediction, we could visualize the regression coefficients. The problem is that since we have fitted the model using the transformed set of features ```z``` the corresponding regression coefficients are not directly interpretable. However, since the ```z``` are linear combinations of the original sMRI features, it is possible to transform the regression coefficients of ```z``` back to the original sMRI features  using the ```coeff.transform``` function from the ```dimreduce``` package.
```{r}
e <- extract(fit$stanfit) # fetch the posterior draws from the stanfit-object
temp <- coeff.transform(dr, t(e$beta[,7:26]), e$alpha)
betax <- cbind(e$beta[,1:6], t(temp$beta)) # coefficients in the x-space
```

Plot the posterior mean of each coefficient
```{r, fig.height=3, fig.width=7}
qplot(1:(ncol(x1)+ncol(x2)), colMeans(betax)) + geom_hline(yintercept = 0, linetype=2) + xlab('Feature') + ylab('Coefficient')
qplot(1:(ncol(x1)+ncol(x2)), colMeans(betax)) + geom_hline(yintercept = 0, linetype=2) + xlab('Feature') + ylab('Coefficient') + ylim(-0.2,0.2)
```

The first plot shows that the first feature ('id1') is clearly the most influential. The zoomed plot shows that there are also plenty of other features that have a clearly nonzero coefficient. Subsequent features seem to have similar coefficients, most likely due to correlations between the features.
We could also plot the marginal distribution for the coefficient of a given variable, such as the gender:
```{r, message=F, fig.width=4, fig.height=3}
xname <- 'gender'
j <- which(colnames(x1)==xname)
qplot(betax[,j]) + xlab(paste0('Coefficient for ', xname))
```








## References 

Piironen, Juho and Vehtari, Aki (2017c). Sparsity information and regularization in the horseshoe and other shrinkage priors. _Electronic Journal of Statistics_, 11(2): 5018--5051. [Online](https://projecteuclid.org/euclid.ejs/1513306866#info)

Piironen, J. and Vehtari, A. (2018). Iterative supervised principal components. In _Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS)_, PMLR 84: 106-114. ([Online](https://proceedings.mlr.press/v84/piironen18a.html)


